{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def decode(dev_data):\n",
    "    dev_contexts_index = []\n",
    "    dev_questions = []\n",
    "    dev_que_id = []\n",
    "    dev_map = {}\n",
    "    for i,line in enumerate(dev_data):\n",
    "        dev_contexts_index.append(line[0])\n",
    "        dev_questions.append(line[1])\n",
    "        dev_que_id.append(line[2])\n",
    "        dev_candidates = []\n",
    "        for j in range(3,len(line),3):\n",
    "            dev_candidates.append((line[j],line[j+1],line[j+2]))\n",
    "        dev_map[i]=dev_candidates\n",
    "    return dev_contexts_index,dev_questions,dev_que_id,dev_map\n",
    "\n",
    "# train set has no id issue comes with a new function\n",
    "def trn_decode(trn_data):\n",
    "    trn_contexts_index = []\n",
    "    trn_questions = []\n",
    "    trn_map = {}\n",
    "    for i,line in enumerate(trn_data):\n",
    "        trn_contexts_index.append(line[0])\n",
    "        trn_questions.append(line[1])\n",
    "        trn_candidates = []\n",
    "        for j in range(2,len(line),3):\n",
    "            trn_candidates.append((line[j],line[j+1],line[j+2]))\n",
    "        trn_map[i]=trn_candidates\n",
    "    return trn_contexts_index,trn_questions,trn_map\n",
    "\n",
    "import csv\n",
    "dev_data = []\n",
    "trn_data = []\n",
    "tst_data = []\n",
    "with open('dev_data.csv') as Dev_DataFile:\n",
    "    raw_dev = csv.reader(Dev_DataFile)\n",
    "    for row in raw_dev:\n",
    "        dev_data.append(row)\n",
    "\n",
    "with open('train_data.csv') as Trn_DataFile:\n",
    "    raw_trn = csv.reader(Trn_DataFile)\n",
    "    for row in raw_trn:\n",
    "#         print(len(row))\n",
    "        trn_data.append(row)\n",
    "\n",
    "with open('test_data.csv') as Tst_DataFile:\n",
    "    raw_tst = csv.reader(Tst_DataFile)\n",
    "    for row in raw_tst:\n",
    "#         print(len(row))\n",
    "        tst_data.append(row)\n",
    "\n",
    "# get all useful list info of each dataset\n",
    "dev_contexts_index, dev_questions, dev_que_id, dev_map = decode(dev_data)\n",
    "tst_contexts_index, tst_questions, tst_que_id, tst_map = decode(tst_data)\n",
    "trn_contexts_index, trn_questions, trn_map = trn_decode(trn_data)\n",
    "\n",
    "# vocabulary processing and collection of each dataset contexts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "with open('dev_contexts.txt') as f:\n",
    "    dev_contexts = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "dev_contexts = [x.strip() for x in dev_contexts] \n",
    "\n",
    "with open('train_contexts.txt') as f:\n",
    "    train_contexts = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "train_contexts = [x.strip() for x in train_contexts]\n",
    "\n",
    "with open('test_contexts.txt') as f:\n",
    "    test_contexts = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "test_contexts = [x.strip() for x in test_contexts]\n",
    "\n",
    "dev_vec = TfidfVectorizer()\n",
    "dev_text_vec = vec.fit_transform(dev_contexts)\n",
    "dev_voc = vec.vocabulary_\n",
    "\n",
    "tst_vec = TfidfVectorizer()\n",
    "tst_text_vec = vec.fit_transform(test_contexts)\n",
    "tst_voc = vec.vocabulary_\n",
    "\n",
    "trn_vec = TfidfVectorizer()\n",
    "trn_text_vec = vec.fit_transform(train_contexts)\n",
    "trn_voc = vec.vocabulary_\n",
    "\n",
    "# Train Matching Word Frequencies\n",
    "Trn_MWF = []\n",
    "for i in trn_map.keys():\n",
    "    for j in trn_map[i]:\n",
    "        trn_wmf_feq_sum = 0\n",
    "        for word in j[0].split():\n",
    "            if word in trn_questions[i].split():\n",
    "                if word.lower() in trn_voc:\n",
    "                    index = trn_voc[word.lower()]\n",
    "                    trn_wmf_feq_sum += trn_text_vec[trn_contexts_index[i],index]\n",
    "        Trn_MWF.append(trn_wmf_feq_sum)\n",
    "\n",
    "# Train Span Word Frequencies\n",
    "Trn_SWF = []\n",
    "for i in trn_map.keys():\n",
    "    for j in trn_map[i]:\n",
    "        trn_swf_feq_sum = 0\n",
    "        for word in j[0].split():\n",
    "            if word.lower() in trn_voc:\n",
    "                index = trn_voc[word.lower()]\n",
    "                trn_swf_feq_sum += trn_text_vec[trn_contexts_index[i],index]\n",
    "        Trn_SWF.append(trn_swf_feq_sum)    \n",
    "\n",
    "# Development Matching Word Frequencies\n",
    "Dev_MWF = []\n",
    "for i in dev_map.keys():\n",
    "    for j in dev_map[i]:\n",
    "        dev_wmf_feq_sum = 0\n",
    "        for word in j[0].split():\n",
    "            if word in dev_questions[i].split():\n",
    "                if word.lower() in dev_voc:\n",
    "                    index = dev_voc[word.lower()]\n",
    "                    dev_wmf_feq_sum += dev_text_vec[dev_contexts_index[i],index]\n",
    "    Dev_MWF.append(dev_wmf_feq_sum)\n",
    "\n",
    "# Development Span Word Frequencies\n",
    "Dev_SWF = []\n",
    "for i in dev_map.keys():\n",
    "    for j in dev_map[i]:\n",
    "        dev_swf_feq_sum = 0\n",
    "        for word in j[0].split():\n",
    "            if word.lower() in dev_voc:\n",
    "                index = dev_voc[word.lower()]\n",
    "                dev_swf_feq_sum += dev_text_vec[dev_contexts_index[i],index]\n",
    "        Dev_SWF.append(dev_swf_feq_sum)    \n",
    "\n",
    "# Test Matching Word Frequencies\n",
    "Tst_MWF = []\n",
    "for i in tst_map.keys():\n",
    "    for j in tst_map[i]:\n",
    "        tst_wmf_feq_sum = 0\n",
    "        for word in j[0].split():\n",
    "            if word in tst_questions[i].split():\n",
    "                if word.lower() in tst_voc:\n",
    "                    index = tst_voc[word.lower()]\n",
    "                    tst_wmf_feq_sum += tst_text_vec[tst_contexts_index[i],index]\n",
    "        Tst_MWF.append(tst_wmf_feq_sum)\n",
    "\n",
    "# Test Span Word Frequencies\n",
    "Tst_SWF = []\n",
    "for i in tst_map.keys():\n",
    "    for j in tst_map[i]:\n",
    "        tst_swf_feq_sum = 0\n",
    "        for word in j[0].split():\n",
    "            if word.lower() in tst_voc:\n",
    "                index = tst_voc[word.lower()]\n",
    "                tst_swf_feq_sum += tst_text_vec[tst_contexts_index[i],index]\n",
    "        Tst_SWF.append(tst_swf_feq_sum) \n",
    "\n",
    "\n",
    "thefile = open('Trn_MWF.txt', 'w')\n",
    "for item in Trn_MWF:\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "thefile = open('Trn_SWF.txt', 'w')\n",
    "for item in Trn_SWF:\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "thefile = open('Dev_MWF.txt', 'w')\n",
    "for item in Dev_MWF:\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "thefile = open('Dev_SWF.txt', 'w')\n",
    "for item in Dev_SWF:\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "thefile = open('Tst_MWF.txt', 'w')\n",
    "for item in Tst_MWF:\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "thefile = open('Tst_SWF.txt', 'w')\n",
    "for item in Tst_SWF:\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "\n",
    "# Lengths\n",
    "dev_left_length = []\n",
    "tst_left_length = []\n",
    "trn_left_length = []\n",
    "dev_inside_length = []\n",
    "tst_inside_length = []\n",
    "trn_inside_length = []\n",
    "dev_whole_length = []\n",
    "tst_whole_length = []\n",
    "trn_whole_length = []\n",
    "\n",
    "for i in dev_map.keys():\n",
    "    for j in dev_map[i]:\n",
    "        if j[1] in j[0]:\n",
    "            index = j[0].index(j[1])\n",
    "        else:\n",
    "            index = len(j[0])//2\n",
    "        left = len(j[0][:index].split())\n",
    "        inside = len(j[1].split())\n",
    "        whole = len(j[0].split())\n",
    "        dev_left_length.append(left)\n",
    "        dev_inside_length.append(inside)\n",
    "        dev_whole_length.append(whole)\n",
    "\n",
    "for i in tst_map.keys():\n",
    "    for j in tst_map[i]:\n",
    "        if j[1] in j[0]:\n",
    "            index = j[0].index(j[1])\n",
    "        else:\n",
    "            index = len(j[0])//2\n",
    "        left = len(j[0][:index].split())\n",
    "        inside = len(j[1].split())\n",
    "        whole = len(j[0].split())\n",
    "        tst_left_length.append(left)\n",
    "        tst_inside_length.append(inside)\n",
    "        tst_whole_length.append(whole)\n",
    "\n",
    "for i in trn_map.keys():\n",
    "    for j in trn_map[i]:\n",
    "        if j[1] in j[0]:\n",
    "            index = j[0].index(j[1])\n",
    "        else:\n",
    "            index = len(j[0])//2\n",
    "        left = len(j[0][:index].split())\n",
    "        inside = len(j[1].split())\n",
    "        whole = len(j[0].split())\n",
    "        trn_left_length.append(left)\n",
    "        trn_inside_length.append(inside)\n",
    "        trn_whole_length.append(whole)\n",
    "        \n",
    "# all length array concatenation\n",
    "trn_length = [trn_left_length, trn_inside_length, trn_whole_length]\n",
    "dev_length = [dev_left_length, dev_inside_length, dev_whole_length]\n",
    "tst_length = [tst_left_length, tst_inside_length, tst_whole_length]\n",
    "# all length array transform\n",
    "trn_length_list=[[r[col] for r in trn_length] for col in range(len(trn_length[0]))]\n",
    "dev_length_list=[[r[col] for r in dev_length] for col in range(len(dev_length[0]))]\n",
    "tst_length_list=[[r[col] for r in tst_length] for col in range(len(tst_length[0]))]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "trn_label  = []\n",
    "for i in trn_map.keys():\n",
    "    for j in trn_map[i]:\n",
    "        trn_label.append(j[2])\n",
    "\n",
    "dev_label  = []\n",
    "for i in dev_map.keys():\n",
    "    for j in dev_map[i]:\n",
    "        dev_label.append(j[2])\n",
    "\n",
    "tst_label  = []\n",
    "for i in tst_map.keys():\n",
    "    for j in tst_map[i]:\n",
    "        tst_label.append(j[2])\n",
    "\n",
    "# extracted feature concatenation\n",
    "trn_feature_list = [trn_mwf, float_trn_swf, trn_left_length, trn_inside_length, trn_whole_length]\n",
    "dev_feature_list = [dev_mwf, dev_swf, dev_left_length, dev_inside_length, dev_whole_length]\n",
    "tst_feature_list = [tst_mwf, tst_swf, tst_left_length, tst_inside_length, tst_whole_length]\n",
    "\n",
    "# all length array transform\n",
    "fixed_trn_feature_list=[[r[col] for r in trn_feature_list] for col in range(len(trn_feature_list[0]))]\n",
    "fixed_dev_feature_list=[[r[col] for r in dev_feature_list] for col in range(len(dev_feature_list[0]))]\n",
    "fixed_tst_feature_list=[[r[col] for r in tst_feature_list] for col in range(len(tst_feature_list[0]))]\n",
    "\n",
    "dev_candidate_label = []\n",
    "tst_candidate_label = []\n",
    "for i, line in enumerate(dev_data):\n",
    "    for j in range(3,len(line),3):\n",
    "        dev_candidate_label.append(i)\n",
    "        \n",
    "for i, line in enumerate(tst_data):\n",
    "    for j in range(3,len(line),3):\n",
    "        tst_candidate_label.append(i)\n",
    "        \n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(trn_length_list, trn_label)\n",
    "dev_output = clf.predict_proba(dev_length_list)\n",
    "tst_output = clf.predict_proba(tst_length_list)\n",
    "\n",
    "dev_max_output = []\n",
    "for i, line in enumerate (dev_output):\n",
    "    dev_max_output.append(max(line))\n",
    "\n",
    "tst_max_output = []\n",
    "for i, line in enumerate (tst_output):\n",
    "    tst_max_output.append(max(line))\n",
    "\n",
    "dev_result = [dev_candidate_label, dev_max_output]\n",
    "tst_result = [tst_candidate_label, tst_max_output]\n",
    "\n",
    "def get_dic_from_two_lists(keys, values):\n",
    "    return { keys[i] : values[i] for i in range(len(keys)) }\n",
    "\n",
    "fixed_dev_result = [[r[col] for r in dev_result] for col in range(len(dev_result[0]))]\n",
    "fixed_tst_result = [[r[col] for r in tst_result] for col in range(len(tst_result[0]))]\n",
    "\n",
    "dev_temp_max = 0\n",
    "tst_temp_max = 0\n",
    "dev_max_candidate_index = 0\n",
    "tst_max_candidate_index = 0\n",
    "dev_que_num = 0\n",
    "tst_que_num = 0\n",
    "dev_count = 0\n",
    "tst_count = 0\n",
    "dev_max_candidate_index_list = []\n",
    "tst_max_candidate_index_list = []\n",
    "\n",
    "for i, line in enumerate (fixed_dev_result):\n",
    "    if line[0] == dev_que_num:\n",
    "        if line[1] >= dev_temp_max:\n",
    "            dev_temp_max = line[1]\n",
    "            dev_max_candidate_index = dev_count\n",
    "        dev_count += 1\n",
    "    else: \n",
    "        dev_que_num += 1\n",
    "        dev_temp_max = 0\n",
    "        dev_count = 0\n",
    "        dev_max_candidate_index_list.append(dev_max_candidate_index)\n",
    "\n",
    "for i, line in enumerate (fixed_tst_result):\n",
    "    if line[0] == tst_que_num:\n",
    "        if line[1] >= tst_temp_max:\n",
    "            tst_temp_max = line[1]\n",
    "            tst_max_candidate_index = tst_count\n",
    "        tst_count += 1\n",
    "    else: \n",
    "        tst_que_num += 1\n",
    "        tst_temp_max = 0\n",
    "        tst_count = 0\n",
    "        tst_max_candidate_index_list.append(tst_max_candidate_index)        \n",
    "\n",
    "dev_answer_list = []\n",
    "tst_answer_list = []\n",
    "\n",
    "for i in range (len(dev_map) - 1):\n",
    "    line = dev_map[i]\n",
    "    index = dev_max_candidate_index_list[i]\n",
    "    dev_answer_list.append(line[index][1])\n",
    "\n",
    "for i in range (len(tst_map) - 1):\n",
    "    line = tst_map[i]\n",
    "    index = tst_max_candidate_index_list[i]\n",
    "    tst_answer_list.append(line[index][1])\n",
    "\n",
    "dev_answer_list.append('NFL')\n",
    "tst_answer_list.append('NFL')\n",
    "\n",
    "dev_pred_dict = get_dic_from_two_lists(dev_que_id, dev_answer_list)\n",
    "tst_pred_dict = get_dic_from_two_lists(tst_que_id, tst_answer_list)\n",
    "with open('dev_prediction_v3.json', 'w') as f:\n",
    "    json.dump(dev_pred_dict, f)\n",
    "with open('tst_prediction_v3.json', 'w') as f:\n",
    "    json.dump(tst_pred_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
