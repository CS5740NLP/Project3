{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "with open('training.json') as json_file:  \n",
    "    train_json = json.load(json_file)\n",
    "with open('development.json') as json_file:  \n",
    "    dev_json = json.load(json_file)\n",
    "with open('testing.json') as json_file:  \n",
    "    test_json = json.load(json_file)\n",
    "with open('sample.json') as json_file:  \n",
    "    sample_json = json.load(json_file)\n",
    "    \n",
    "def get_dic_from_two_lists(keys, values):\n",
    "    return { keys[i] : values[i] for i in range(len(keys)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for article in dev_json['data']:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for qa in paragraph['qas']:\n",
    "            context = paragraph['context']\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "            question = qa['question']\n",
    "            answer = qa['answers'][0]\n",
    "            ans_text = answer['text']\n",
    "            ans_start = answer['answer_start']\n",
    "            ans_end = answer['answer_start']+len(ans_text)\n",
    "            data.append((context,question,ans_text,ans_start,ans_end))\n",
    "            \n",
    "contexts = [a for (a,b,c,d,e) in data]\n",
    "questions = [b for (a,b,c,d,e) in data]\n",
    "answers = [c for (a,b,c,d,e) in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def embedding(sequence,embeddings_index):\n",
    "    max_len_func = lambda x: np.array([len(x) for x in sequence]).max()\n",
    "    max_len = max_len_func(sequence)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sequence)\n",
    "    sequences = tokenizer.texts_to_sequences(sequence)\n",
    "    word_index = tokenizer.word_index\n",
    "    data = pad_sequences(sequences, maxlen=max_len)\n",
    "    \n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix,max_len,data,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f: \n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "q_embed = embedding(questions,embeddings_index)\n",
    "c_embed = embedding(contexts,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer_1 = Embedding(len(q_embed[3]) + 1,\n",
    "                            100,\n",
    "                            weights=[q_embed[0]],\n",
    "                            input_length=q_embed[1],\n",
    "                            trainable=False)\n",
    "embedding_layer_2 = Embedding(len(c_embed[3]) + 1,\n",
    "                            100,\n",
    "                            weights=[c_embed[0]],\n",
    "                            input_length=c_embed[1],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contexts_max_length = np.array([len(x) for x in contexts]).max()\n",
    "answer_one_hot = np.zeros((len(answers),contexts_max_length))\n",
    "for i in range(len(contexts)):\n",
    "    a_l = len(answers[i])\n",
    "    for j in range(len(contexts[i])-a_l+1):\n",
    "        if contexts[i][j]==answers[i][0] and contexts[i][j+a_l-1]==answers[i][a_l-1]:\n",
    "            start_ix = j\n",
    "            end_ix = j+a_l-1\n",
    "            answer_one_hot[i][[start_ix,end_ix]]=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.layers import concatenate\n",
    "\n",
    "questions_input = Input(shape=(q_embed[1],))\n",
    "embedded_questions = embedding_layer_1(questions_input)\n",
    "_, state_h_q,state_c_q  = LSTM(80,return_state=True)(embedded_questions)\n",
    "contexts_input = Input(shape=(c_embed[1],))\n",
    "embedded_contexts = embedding_layer_2(contexts_input)\n",
    "_, state_h_c, state_c_c = LSTM(80,return_state=True)(embedded_contexts)\n",
    "H = concatenate([state_h_c,state_h_q,state_c_c,state_c_q])\n",
    "preds = Dense(answer_one_hot.shape[1], activation='softmax')(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model([contexts_input,questions_input], preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='RMSprop',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "x_train = [c_embed[2],q_embed[2]]\n",
    "y_train = answer_one_hot\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=2, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
